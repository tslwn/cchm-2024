\addsection{Choosing Your Evidence: Freeing but Fraught}

\sectionauthor[]{Deanna Kuhn}
\begin{affils}
  \sectionaffil[]{Columbia University}
\end{affils}

I haven't ventured into AI modelling, but have instead studied what we call `causal
cognition' in the wild, e.g., children, adolescents, and adults.
I began by using the paradigm that's typical of much causal-inference research, i.e.,
here's some kind of data display, what can you conclude?
More recently, I began to wonder if this is really the best paradigm for causal
cognition, especially in the wild -- is that what we really do?
I would argue maybe not.
Considering much of what we read these days, and can't escape, we think `how can $X$
say that'!
Or, if you have your own favourite claim, you think about what evidence supports it.
I would describe that as a more typical form of causal cognition in everyday
circumstances.
Therefore, I have moved towards this paradigm in my design.
We've used this kind of paradigm with children, adolescents, and adults.
We drew on a favourite topic: obesity.
I've been impressed of the importance of causal cognition in a multivariate format, and
to get away from the `school science', univariate kind of approach.
If there were ever a topic where multiple causes contribute to an outcome, obesity is
one.

Tables of three variables (exercise, diet, and parent).
The subjects weren't shown the tables in this format.
What's your theory?
What evidence would you need to show that you're right?
What we actually presented to the subjects was a set of data-cards, separately, which
were movable, so the subjects could make comparisons, etc. `You want to demonstrate
that (e.g.) exercise matters -- what cards do you need to show'?
How many cells in the four-cell table did people need to make reference to (four
cards).
How many students felt they needed lots of cards -- only 1 out of 35, despite them
being students with a good socioeconomic status and education.

Showed that the ability to reason proportionally wasn't the problem.
People didn't have trouble with that.
Young children just use the biggest quantity rather than the biggest fraction.
But the 35 in the adolescent sample didn't have that problem.
Make the case (without getting into it) -- the issue is that it's a cognitive laziness
kind of phenomenon, rather than a competence deficit, as with younger children.
Think you can extend that to the stance of cognition in natural settings.
What's the minimum that would make my case?
Make the extension that's what we see in more consequential reasoning.

Moved on to allow participants to do something that they couldn't do in the previous
study, which is to look at interaction effects.
That's where the real answer lies.

Two take-home messages: what happens when you give people a richer database that allows
people to reason about multiple causes?
Elicited explanations and asked people with the same design what evidence they'd need.
What they did was common-sensical, i.e., look at the output variable.
Controlled comparison is perhaps not the answer for science education.
Left explanation out of the picture -- variable at the bottom.
Proportion of belief revision increased with pre- and post-data explanation, less so
with post-data explanation, almost none with no explanation.
Explanation is a double-edged sword: some is helpful, too much has adverse effects.
Story can help people to attend to the data more.
